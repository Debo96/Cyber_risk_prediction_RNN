{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import string \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('C:/Users/ABC/Desktop/CVE_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('C:/Users/ABC/Desktop/CVE_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "train['cvss_review']= label_encoder.fit_transform(train['cvss_review']) \n",
    "  \n",
    "train['cvss_review'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mod_date</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>cvss</th>\n",
       "      <th>cwe_code</th>\n",
       "      <th>cwe_name</th>\n",
       "      <th>summary</th>\n",
       "      <th>cvss_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CVE-2019-16548</td>\n",
       "      <td>11/21/2019 15:15</td>\n",
       "      <td>11/21/2019 15:15</td>\n",
       "      <td>6.8</td>\n",
       "      <td>352</td>\n",
       "      <td>Cross-Site Request Forgery (CSRF)</td>\n",
       "      <td>A cross-site request forgery vulnerability in ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CVE-2019-16547</td>\n",
       "      <td>11/21/2019 15:15</td>\n",
       "      <td>11/21/2019 15:15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>732</td>\n",
       "      <td>Incorrect Permission Assignment for Critical ...</td>\n",
       "      <td>Missing permission checks in various API endpo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CVE-2019-16546</td>\n",
       "      <td>11/21/2019 15:15</td>\n",
       "      <td>11/21/2019 15:15</td>\n",
       "      <td>4.3</td>\n",
       "      <td>639</td>\n",
       "      <td>Authorization Bypass Through User-Controlled Key</td>\n",
       "      <td>Jenkins Google Compute Engine Plugin 4.1.1 and...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CVE-2013-2092</td>\n",
       "      <td>11/20/2019 21:22</td>\n",
       "      <td>11/20/2019 21:15</td>\n",
       "      <td>4.3</td>\n",
       "      <td>79</td>\n",
       "      <td>Improper Neutralization of Input During Web P...</td>\n",
       "      <td>Cross-site Scripting (XSS) in Dolibarr ERP/CRM...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CVE-2013-2091</td>\n",
       "      <td>11/20/2019 20:15</td>\n",
       "      <td>11/20/2019 20:15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>89</td>\n",
       "      <td>Improper Neutralization of Special Elements u...</td>\n",
       "      <td>SQL injection vulnerability in Dolibarr ERP/CR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1496</td>\n",
       "      <td>CVE-2019-8084</td>\n",
       "      <td>10/28/2019 12:30</td>\n",
       "      <td>10/25/2019 15:15</td>\n",
       "      <td>4.3</td>\n",
       "      <td>79</td>\n",
       "      <td>Improper Neutralization of Input During Web P...</td>\n",
       "      <td>Adobe Experience Manager versions 6.5, 6.4, 6....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1497</td>\n",
       "      <td>CVE-2019-8083</td>\n",
       "      <td>10/28/2019 12:23</td>\n",
       "      <td>10/25/2019 15:15</td>\n",
       "      <td>4.3</td>\n",
       "      <td>79</td>\n",
       "      <td>Improper Neutralization of Input During Web P...</td>\n",
       "      <td>Adobe Experience Manager versions 6.5, 6.4 and...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1498</td>\n",
       "      <td>CVE-2019-17133</td>\n",
       "      <td>10/28/2019 2:15</td>\n",
       "      <td>10/4/2019 12:15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>120</td>\n",
       "      <td>Buffer Copy without Checking Size of Input ('...</td>\n",
       "      <td>In the Linux kernel through 5.3.2, cfg80211_mg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>1499</td>\n",
       "      <td>CVE-2015-9503</td>\n",
       "      <td>10/28/2019 1:11</td>\n",
       "      <td>10/23/2019 17:15</td>\n",
       "      <td>4.3</td>\n",
       "      <td>79</td>\n",
       "      <td>Improper Neutralization of Input During Web P...</td>\n",
       "      <td>The Modern theme before 1.4.2 for WordPress ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1500</td>\n",
       "      <td>CVE-2015-9502</td>\n",
       "      <td>10/28/2019 1:11</td>\n",
       "      <td>10/23/2019 17:15</td>\n",
       "      <td>4.3</td>\n",
       "      <td>79</td>\n",
       "      <td>Improper Neutralization of Input During Web P...</td>\n",
       "      <td>The Auberge theme before 1.4.5 for WordPress h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      Unnamed: 0          mod_date          pub_date  cvss  \\\n",
       "0        1  CVE-2019-16548  11/21/2019 15:15  11/21/2019 15:15   6.8   \n",
       "1        2  CVE-2019-16547  11/21/2019 15:15  11/21/2019 15:15   4.0   \n",
       "2        3  CVE-2019-16546  11/21/2019 15:15  11/21/2019 15:15   4.3   \n",
       "3        4   CVE-2013-2092  11/20/2019 21:22  11/20/2019 21:15   4.3   \n",
       "4        5   CVE-2013-2091  11/20/2019 20:15  11/20/2019 20:15   7.5   \n",
       "...    ...             ...               ...               ...   ...   \n",
       "1495  1496   CVE-2019-8084  10/28/2019 12:30  10/25/2019 15:15   4.3   \n",
       "1496  1497   CVE-2019-8083  10/28/2019 12:23  10/25/2019 15:15   4.3   \n",
       "1497  1498  CVE-2019-17133   10/28/2019 2:15   10/4/2019 12:15   7.5   \n",
       "1498  1499   CVE-2015-9503   10/28/2019 1:11  10/23/2019 17:15   4.3   \n",
       "1499  1500   CVE-2015-9502   10/28/2019 1:11  10/23/2019 17:15   4.3   \n",
       "\n",
       "      cwe_code                                           cwe_name  \\\n",
       "0          352                  Cross-Site Request Forgery (CSRF)   \n",
       "1          732   Incorrect Permission Assignment for Critical ...   \n",
       "2          639   Authorization Bypass Through User-Controlled Key   \n",
       "3           79   Improper Neutralization of Input During Web P...   \n",
       "4           89   Improper Neutralization of Special Elements u...   \n",
       "...        ...                                                ...   \n",
       "1495        79   Improper Neutralization of Input During Web P...   \n",
       "1496        79   Improper Neutralization of Input During Web P...   \n",
       "1497       120   Buffer Copy without Checking Size of Input ('...   \n",
       "1498        79   Improper Neutralization of Input During Web P...   \n",
       "1499        79   Improper Neutralization of Input During Web P...   \n",
       "\n",
       "                                                summary  cvss_review  \n",
       "0     A cross-site request forgery vulnerability in ...            2  \n",
       "1     Missing permission checks in various API endpo...            2  \n",
       "2     Jenkins Google Compute Engine Plugin 4.1.1 and...            2  \n",
       "3     Cross-site Scripting (XSS) in Dolibarr ERP/CRM...            2  \n",
       "4     SQL injection vulnerability in Dolibarr ERP/CR...            0  \n",
       "...                                                 ...          ...  \n",
       "1495  Adobe Experience Manager versions 6.5, 6.4, 6....            2  \n",
       "1496  Adobe Experience Manager versions 6.5, 6.4 and...            2  \n",
       "1497  In the Linux kernel through 5.3.2, cfg80211_mg...            0  \n",
       "1498  The Modern theme before 1.4.2 for WordPress ha...            2  \n",
       "1499  The Auberge theme before 1.4.5 for WordPress h...            2  \n",
       "\n",
       "[1500 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a cross-site request forgery vulnerability in ...\n",
       "1       missing permission checks in various api endpo...\n",
       "2       jenkins google compute engine plugin 4.1.1 and...\n",
       "3       cross-site scripting (xss) in dolibarr erp/crm...\n",
       "4       sql injection vulnerability in dolibarr erp/cr...\n",
       "                              ...                        \n",
       "1495    adobe experience manager versions 6.5, 6.4, 6....\n",
       "1496    adobe experience manager versions 6.5, 6.4 and...\n",
       "1497    in the linux kernel through 5.3.2, cfg80211_mg...\n",
       "1498    the modern theme before 1.4.2 for wordpress ha...\n",
       "1499    the auberge theme before 1.4.5 for wordpress h...\n",
       "Name: summary, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['summary'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [A,  , c, r, -, e,  , r, e, q, u, e,  , f, r, ...\n",
       "1       [M, n, g,  , p, e, r, n,  , c, h, e, c, k,  , ...\n",
       "2       [J, e, n, k, n,  , G, g, l, e,  , C, p, u, e, ...\n",
       "3       [C, r, -, e,  , S, c, r, p, n, g,  , (, X, S, ...\n",
       "4       [S, Q, L,  , n, j, e, c, n,  , v, u, l, n, e, ...\n",
       "                              ...                        \n",
       "1495    [A, b, e,  , E, x, p, e, r, e, n, c, e,  , M, ...\n",
       "1496    [A, b, e,  , E, x, p, e, r, e, n, c, e,  , M, ...\n",
       "1497    [I, n,  , h, e,  , L, n, u, x,  , k, e, r, n, ...\n",
       "1498    [T, h, e,  , M, e, r, n,  , h, e, e,  , b, e, ...\n",
       "1499    [T, h, e,  , A, u, b, e, r, g, e,  , h, e, e, ...\n",
       "Name: summary, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "train['summary'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [a,  , c, r, o, s, s, -, s, i, t, e,  , r, e, ...\n",
       "1       [m, i, s, s, i, n, g,  , p, e, r, m, i, s, s, ...\n",
       "2       [j, e, n, k, i, n, s,  , g, o, o, g, l, e,  , ...\n",
       "3       [c, r, o, s, s, -, s, i, t, e,  , s, c, r, i, ...\n",
       "4       [s, q, l,  , i, n, j, e, c, t, i, o, n,  , v, ...\n",
       "                              ...                        \n",
       "1495    [a, d, o, b, e,  , e, x, p, e, r, i, e, n, c, ...\n",
       "1496    [a, d, o, b, e,  , e, x, p, e, r, i, e, n, c, ...\n",
       "1497    [i, n,  , t, h, e,  , l, i, n, u, x,  , k, e, ...\n",
       "1498    [t, h, e,  , m, o, d, e, r, n,  , t, h, e, m, ...\n",
       "1499    [t, h, e,  , a, u, b, e, r, g, e,  , t, h, e, ...\n",
       "Name: summary, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "train['summary'].apply(lambda x: [stemmer.stem(e) for e in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [A,  , c, r, o, s, s, -, s, i, t, e,  , r, e, ...\n",
       "1       [M, i, s, s, i, n, g,  , p, e, r, m, i, s, s, ...\n",
       "2       [J, e, n, k, i, n, s,  , G, o, o, g, l, e,  , ...\n",
       "3       [C, r, o, s, s, -, s, i, t, e,  , S, c, r, i, ...\n",
       "4       [S, Q, L,  , i, n, j, e, c, t, i, o, n,  , v, ...\n",
       "                              ...                        \n",
       "1495    [A, d, o, b, e,  , E, x, p, e, r, i, e, n, c, ...\n",
       "1496    [A, d, o, b, e,  , E, x, p, e, r, i, e, n, c, ...\n",
       "1497    [I, n,  , t, h, e,  , L, i, n, u, x,  , k, e, ...\n",
       "1498    [T, h, e,  , M, o, d, e, r, n,  , t, h, e, m, ...\n",
       "1499    [T, h, e,  , A, u, b, e, r, g, e,  , t, h, e, ...\n",
       "Name: summary, Length: 1500, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train['summary'].apply(lambda x: [lemmatizer.lemmatize(e) for e in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5954 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 1500\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train['summary'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1500, 250)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = tokenizer.texts_to_sequences(train['summary'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 138,   2, 508],\n",
       "       [  0,   0,   0, ..., 181,   8, 888],\n",
       "       [  0,   0,   0, ...,   1, 584, 212],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   3, 125, 129],\n",
       "       [  0,   0,   0, ...,   1, 577, 148],\n",
       "       [  0,   0,   0, ...,   1, 577, 148]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=train.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 16s 11ms/step - loss: 1.0240 - acc: 0.5780\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 16s 10ms/step - loss: 0.8851 - acc: 0.6140\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.8230 - acc: 0.6447\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 0.7058 - acc: 0.6947\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 0.5768 - acc: 0.7673\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.4898 - acc: 0.8047\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.3979 - acc: 0.8500\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.3340 - acc: 0.8787\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 17s 11ms/step - loss: 0.2923 - acc: 0.8973\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 17s 12ms/step - loss: 0.2715 - acc: 0.9047\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.2079 - acc: 0.9320\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.1809 - acc: 0.9407\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.1565 - acc: 0.9460\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.1500 - acc: 0.9473\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.1441 - acc: 0.9567\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 0.1365 - acc: 0.9493\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.1122 - acc: 0.9680\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.1061 - acc: 0.9607\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0959 - acc: 0.9733\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0949 - acc: 0.9707\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0893 - acc: 0.9693\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0910 - acc: 0.9707\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0841 - acc: 0.9713\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0818 - acc: 0.9740\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0750 - acc: 0.9780\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0868 - acc: 0.9673\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0709 - acc: 0.9753\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0711 - acc: 0.9767\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0695 - acc: 0.9787\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0768 - acc: 0.9733\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "history = model.fit(X, Y, epochs=epochs, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chatbot_model.h5', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.5946753e-04 2.3951942e-04 9.9920100e-01]] medium_risk\n"
     ]
    }
   ],
   "source": [
    "preceeding_events = ['Integer overflow in bufferobject.c in Python before 2.7.8 allows context-dependent attackers to obtain sensitive information from process memory via a large size and offset in a \"buffer\" function.']\n",
    "seq = tokenizer.texts_to_sequences(preceeding_events)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = ['high_risk','low_risk','medium_risk']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
